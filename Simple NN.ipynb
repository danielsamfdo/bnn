{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/breast-cancer-wisconsin.csv\",sep=',')\n",
    "df = shuffle(df)\n",
    "train_end = 500\n",
    "\n",
    "train_df = df[:train_end]\n",
    "test_df = df[train_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDS(df):\n",
    "    df1 = df.iloc[:,1:10]\n",
    "    df2 = df.iloc[:,10]\n",
    "    y = np.array(df2)\n",
    "    y[y==4]=1 # malignant\n",
    "    y[y==2]=0 # benign\n",
    "    X = np.array(df1)\n",
    "    X = X.astype(float)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = getDS(train_df)\n",
    "test = getDS(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,binary=False):\n",
    "        super(Net, self).__init__()\n",
    "        if(binary):\n",
    "            self.fc1 = NewBinaryLayer(9, 50)\n",
    "            self.fc2 = NewBinaryLayer(50, 2)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(9, 50)\n",
    "            self.fc2 = nn.Linear(50, 2)\n",
    "        self.scores = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.scores(x)\n",
    "        return x\n",
    "\n",
    "def binarize(W, stochastic=False):\n",
    "    x = copy.deepcopy(W.data)\n",
    "    x = torch.clamp((x+1.0)/2.0, 0, 1)\n",
    "\n",
    "    x = torch.clamp((x+1.0)/2.0, 0, 1)\n",
    "    y = copy.deepcopy(x)\n",
    "    x = torch.round(x)\n",
    "    #     print(x),\"HELLLLLLLLLLO\",y\n",
    "    x[x==1] = 1\n",
    "    x[x==0] = -1\n",
    "    return x,y\n",
    "\n",
    "class NewBinaryLayer(nn.Linear):\n",
    "    #initialize the Binary Layer where weights are binarized\n",
    "    def __init__(self, input_dim, output_dim, verbose=False):\n",
    "        self.verbose = verbose\n",
    "        super(NewBinaryLayer, self).__init__(input_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        print \"Weights,bias in forward prop before binarization\"\n",
    "        print self.weight.data\n",
    "        print self.bias.data\n",
    "        \n",
    "        self.new_weight,clipped_wt_data = binarize(self.weight)\n",
    "        if(self.verbose):\n",
    "            print self.weight.data\n",
    "            print self.new_weight\n",
    "        self.weight.data = clipped_wt_data\n",
    "        backup_weight = self.weight.data\n",
    "        self.weight.data = self.new_weight\n",
    "        if(self.verbose):\n",
    "            print \"inputs\"\n",
    "            print x.data\n",
    "\n",
    "            print \"Weights,bias in forward prop after binarization\"\n",
    "            print self.weight.data\n",
    "            print self.bias.data\n",
    "        \n",
    "        out = super(NewBinaryLayer, self).forward(x)\n",
    "        if(self.verbose):\n",
    "            print \"computing wx + b \"\n",
    "            print out\n",
    "\n",
    "        self.weight.data = backup_weight\n",
    "        #          #### CHECK GRADIENTS IN BACKWARD FLOW\n",
    "        #         gradients = torch.FloatTensor([[1.0,0.0]])\n",
    "        #         out.backward(gradients)\n",
    "\n",
    "        #         print(x.grad)\n",
    "        #         print out.grad, self.weight.grad, self.bias.grad, x.grad\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# pla = Net()\n",
    "# pla = BinaryNetwork(9,50,2)\n",
    "# for i, data in enumerate(train_dataloader, 0):\n",
    "\n",
    "#     # get the inputs\n",
    "#     inputs, labels = data['X'], data['classes']\n",
    "# # wrap them in Variable\n",
    "# inputs, labels = Variable(inputs.float()), Variable(labels)\n",
    "# optimizer = optim.SGD(pla.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# # zero the parameter gradients\n",
    "# optimizer.zero_grad()\n",
    "# pla.forward(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CancerDataset(Dataset):\n",
    "    \"\"\"CancerDataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.X.shape[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'X': self.X[idx], 'classes': self.y[idx]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = CancerDataset(train[0],train[1])\n",
    "test_ds = CancerDataset(test[0],test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size=20,shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=20,shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_time(net, train_dataloader):\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    net.train()\n",
    "    \n",
    "    for epoch in range(100):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data['X'], data['classes']\n",
    "            # wrap them in Variable\n",
    "            inputs, labels = Variable(inputs.float()), Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.data[0]\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_time(net, test_dataloader):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.NLLLoss()\n",
    "    for batch_idx, data in enumerate(test_dataloader):\n",
    "        inputs, labels = data['X'], data['classes']\n",
    "        inputs, targets = Variable(inputs.float()), Variable(labels)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "    print \"test set Accuracy\", correct/float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "tnet = Net()\n",
    "tnet = train_time(tnet,train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set Accuracy 0.956\n"
     ]
    }
   ],
   "source": [
    "test_time(tnet,train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set Accuracy 0.945355191257\n"
     ]
    }
   ],
   "source": [
    "test_time(tnet,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "tnet = Net(True)\n",
    "tnet = train_time(tnet,train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set Accuracy 0.956\n"
     ]
    }
   ],
   "source": [
    "test_time(tnet,train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set Accuracy 0.956284153005\n"
     ]
    }
   ],
   "source": [
    "test_time(tnet,test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hard sigmoid functiom\n",
    "def hard_sigmoid(x):\n",
    "    return torch.clamp((x+1.)/2., 0, 1)\n",
    "\n",
    "class BinarizeWeights(torch.autograd.Function):\n",
    "    def __init__(self):\n",
    "        super(BinarizeWeights, self).__init__()\n",
    "    \n",
    "    def forward(self, input, S, stochastic=True):\n",
    "        self.save_for_backward(S)\n",
    "        if(stochastic):\n",
    "            x = hard_sigmoid(input)\n",
    "            res = torch.bernoulli(x)\n",
    "            res[res == 0] = -1\n",
    "        else:\n",
    "            res = torch.sign()\n",
    "        return res\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        S, = self.saved_tensors\n",
    "        grad_input = torch.mm(grad_output, S)\n",
    "        return grad_input\n",
    "\n",
    "    \n",
    "\n",
    "class BinaryLayer(nn.Linear):\n",
    "    #initialize the Binary Layer where weights are binarized\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(BinaryLayer, self).__init__(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.new_weight = BinarizeWeights()(self.weight,x)\n",
    "        # print self.new_weight.grad_fn\n",
    "        backup_weight = self.weight.data\n",
    "        self.weight.data = self.new_weight.data\n",
    "        out = super(BinaryLayer, self).forward(x)\n",
    "        return out\n",
    "\n",
    "class Binaryactivation(torch.autograd.Function):\n",
    "    #initialize the Binary Activation Function after Tanh\n",
    "    def __init__(self):\n",
    "        super(Binaryactivation, self).__init__()\n",
    "        \n",
    "    def forward(self, input, stochastic=True):\n",
    "        self.save_for_backward(input)\n",
    "        if(stochastic):\n",
    "            x = hard_sigmoid(input)\n",
    "            out = torch.bernoulli(x)\n",
    "            out[out == 0] = -1\n",
    "        else:\n",
    "            out = torch.sign(input)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = self.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[torch.abs(input) >= 1] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "    \n",
    "class BinarytanH(torch.autograd.Function):\n",
    "    #initialize the Binary Activation Function after Tanh\n",
    "    def __init__(self):\n",
    "        super(BinarytanH, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = F.tanh(x)\n",
    "        out = Binaryactivation()(res)\n",
    "#         print out.grad_fn\n",
    "        return out\n",
    "\n",
    "\n",
    "class BinaryNetwork(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(BinaryNetwork, self).__init__()\n",
    "        self.linear1 = BinaryLayer(D_in, H)\n",
    "        self.linear2 = BinaryLayer(H, D_out)\n",
    "        self.binarized_tanh = BinarytanH()\n",
    "        self.scores = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        linear_1 = self.linear1(x)\n",
    "        activation_1 = self.binarized_tanh.forward(linear_1)\n",
    "        smax_1 = self.linear2(activation_1)\n",
    "        output = self.scores(smax_1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# binet = BinaryNetwork(9,50,2)\n",
    "# binet = train_time(binet,train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_time(binet,train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, binary):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.binary = binary\n",
    "        if self.binary:\n",
    "            self.fc1 = NewBinaryLayer(2, 2, True)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(2, 2)     \n",
    "        self.fc1.weight.data = torch.FloatTensor([[0.5, 1.5], [-0.5, 1]])\n",
    "        self.fc1.bias.data = torch.FloatTensor([0, 0])\n",
    "        \n",
    "        print \"init weights\"\n",
    "        print self.fc1.weight\n",
    "        print self.fc1.bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        # x = F.tanh(self.fc1(x))\n",
    "#         z = x.mean()\n",
    "#         z.backward()\n",
    "#         print x.grad\n",
    "        return x\n",
    "#         return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weights\n",
      "Parameter containing:\n",
      " 0.5000  1.5000\n",
      "-0.5000  1.0000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Parameter containing:\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "Weights,bias in forward prop before binarization\n",
      "\n",
      " 0.5000  1.5000\n",
      "-0.5000  1.0000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "\n",
      " 1  1\n",
      " 0  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      " HELLLLLLLLLLO \n",
      " 0.7500  1.0000\n",
      " 0.2500  1.0000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "\n",
      " 0.5000  1.5000\n",
      "-0.5000  1.0000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "\n",
      " 1  1\n",
      "-1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "inputs\n",
      "\n",
      " 1  2\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Weights,bias in forward prop after binarization\n",
      "\n",
      " 1  1\n",
      "-1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "computing wx + b \n",
      "Variable containing:\n",
      " 3  1\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "None\n",
      "None Variable containing:\n",
      " 1  2\n",
      " 0  0\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 1\n",
      " 0\n",
      "[torch.FloatTensor of size 2]\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "# Simple Check For Binarize Forward And Backward Propagation : \n",
    "n = LinearNet(True)\n",
    "n.fc1.register_backward_hook(printgradnorm)\n",
    "ip = torch.FloatTensor([[1, 2]])\n",
    "ip = Variable(ip, requires_grad=False)\n",
    "\n",
    "x = n.forward(ip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# super(n).backward(Variable(torch.FloatTensor([[0.5, 1.5], [-0.5, 1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x114597f90>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def printgradnorm(self, grad_input, grad_output):\n",
    "    print('Inside ' + self.__class__.__name__ + ' backward')\n",
    "    print('Inside class:' + self.__class__.__name__)\n",
    "    print('')\n",
    "    print('grad_input: ', type(grad_input))\n",
    "    print('grad_input[0]: ', type(grad_input[0]))\n",
    "    print('grad_output: ', type(grad_output))\n",
    "    print('grad_output[0]: ', type(grad_output[0]))\n",
    "    print('')\n",
    "    print('grad_input size:', grad_input[0].size())\n",
    "    print('grad_output size:', grad_output[0].size())\n",
    "    print('grad_input norm:', grad_input[0].data.norm())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loss = nn.MSELoss()\n",
    "# target = (torch.FloatTensor([1, 0]))\n",
    "# output = loss(n, target)\n",
    "# output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [pytorchnn]",
   "language": "python",
   "name": "Python [pytorchnn]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
